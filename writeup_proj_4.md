# **Behavioral Cloning** 
**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image0]: ./images/Train_valid_loss.png "Train-valid loss"
[image1]: ./images/model.png "Model Visualization"
[image2]: ./examples/placeholder.png "Grayscaling"
[image3]: ./examples/placeholder_small.png "Recovery Image"
[image4]: ./examples/placeholder_small.png "Recovery Image"
[image5]: ./examples/placeholder_small.png "Recovery Image"
[image6]: ./examples/placeholder_small.png "Normal Image"
[image7]: ./examples/placeholder_small.png "Flipped Image"

## Rubric Points
Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_proj_4.md summarizes the results

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable


The model.py is dedicated to produce the model.h5, which contains the NN model for generate the steering command for drive.py.
The model.py includes the following three key parts:
- Model definition
- Processing of traing and validation data
- Model training

**Model Definition**

The model is described and defined using Keras APIs in `create_model_B2()` function

**Processing Data**

The first step of data loading is to load the `driving_log.csv` file as a list. This file contains the files names of input images to NN and the output labels. After loading the `samples` list, I use `train_test_split()` from `sklearn.model_selection` module to ramdonly split the data set into `train_samples` and `validation_samples` in the ratio of 4:1 (80%, 20%).

Instead of directly loading all images into memory at once, I create a generator to load and process image data on the fly.

The basic structure of the data generator is shown in the following code block. Since the rule of generator is to continuously provide random batch to the optimizer, I remove the concept of `epoch` for simplification. This results in an architecture of single-producer-single-consumer type of queues. The producer load images and labels into the queues `images` and `angles`. The augmented data iares also generated by producer and are pushed into the queue, if there is any. The consumer checks if the size of the buffer is greater than `yield_size = 1.5*batch_size`. If the yield condition is satisfied, a portion of the data (`batch_size` of them) in the buffer will be popped and yield to the optimizer.

```python
# The batch generator with augmentaiton
def generator_aug(sample_list, batch_size=32, yield_size=None, data_path="."):
    #
    if yield_size is None:
        yield_size = int(batch_size*1.5)
    #
    images = []
    angles = []
    while True: # Eternal loop
        sample_list = sklearn.utils.shuffle(sample_list)
        #
        # Loop over all samples
        for sample in sample_list:
            current_path_center = data_path + '/IMG/' + sample[0].split('/')[-1]
            image_center = imageio.imread(current_path_center)
            angle_center = float(sample[3])
            # Center
            images.append(image_center)
            angles.append(angle_center)

            # yield
            if len(images) >= yield_size:
                # yield
                # Shuffle
                images, angles = sklearn.utils.shuffle(images, angles)
                # Convert to ndarray
                X_train = np.array(images[:batch_size])
                y_train = np.array(angles[:batch_size])
                # yield sklearn.utils.shuffle(X_train, y_train)
                yield (X_train, y_train)
                images = images[batch_size:]
                angles = angles[batch_size:]
            #

```
 
**Model Trainging**

Since my local computer sill sometimes shuted-down from over-heating, I construct a mechanism for resuming the training status from last unfinished training. This includes two parts:
- `ModelCheckpoint` callback to save temporary models
- `load_model` to load the save temporary model, if any.



### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model consists of four convolution layers, the filter kernal size are all 5x5 (model.py line 274, 278, 281, and 284). Each convolution layers are activated by Leaky-ReLU (model.py line 275, 279, 282, and 285). The input data are normalized by the second layer, which is Keras lambda layer performin the following function `lambda x: x / 255.0 - 0.5` (model.py line 272).


#### 2. Attempts to reduce overfitting in the model

The model contains a dropout layer at the last convolution layer (model.py line 286) and L2 regulizers for weight matrixes of all `Dense` layers.

The model was trained and validated on different data sets (`train_samples` and `validation_samples`) to varify that if the model is overfitted  (code line 49). However, the testing is using the realtime generated data by running the simulator and `drive.py` to how the car drive instead of test with pre-recorded dataset.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 25).

The model was trained by adam optimizer with all the parameters left to default (model.py line 298). The dropout rate for the model is set at 0.5 (model.py line 286). The batch size is set to 32 samples (model.py line 137); however, since the infinitively-looped generator is used, the `train_steps_epoch` is set arbitrarily to `100` (model.py line 357). The chosen of `train_steps_epoch` does not effect the quality of training; instead, it effect how offent the optimizer will validate the model with validation set and how offent the temporary model will be saved. I chose `100` so that the training step can be validated and saved offently.


#### 4. Appropriate training data

The data I used for training is basically the pre-recorded data provided by project. The data provided by project is a center-lane-driving record with center image, right-shifted image, and left-shifted image and the steering angle as label. Instead of collecting new data, I augmented the given dataset by utilizing the left/right images to generate recovery behaviors. Also, to further increase the data utilization, center images, left images, and right images are all mirrored horizontally. By doing so, the model can learn similar behavior for left and right turn or get more data for right/left turn from data of left/right turn.

The details of the data for training are described in the next section.


### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The final version of the structure is illustrated in Fig. 1.

When designing the model, the first intuition I came up with for this task is that the output should be able to provide symmetric value around zero, so that the vehicle can have similar ability to turn left and right. To accomplish this, it's necessary that the last few layer represents an even function repected to each feature/activation from the former layers.

I design the the last 2nd layer to have two output with `(Leaky)ReLU` activation. In this way, the left turn command might be provided by one of the activation, while the right turn command is provided by another one.

When it comes to the last 3rd and 4th layer, however, I realized that the `(Leaky)ReLU` is not efficient for generating turning features, since idealy only half of them will be activated when turning. Hence, I use `tanh` as activation.


The design of the feature generator, i.e. the 'Conv2D' layers, is modified from the Nvidia proposed network for self-driving car. One of the most important concept of the design is that, I think, it reduce the dimension of the output by using stride instead of pooling layer. The location of features are critical in this application, while pooling layers drop information of location and is not suitable.

The resulted network contains 242,084 trainable parameters, and is light to train. The training for each epoch only takes 17 sec. on my local computer (using CPU). 

Fig. 1 shows the training loss and validation loss. It's not clear why the validation loss is smaller than training loss when using `fit_generator()`; however, we still can see the trend.

![alt text][image0]

Fig. 1 Training/validation loss


The final step was to run the simulator to see how well the car was driving around track one. After several tweaking the model design and training for 30 epoch, the car drive well in simulator (track 1).





#### 2. Final Model Architecture

The final model architecture (model.py lines 270-298) is summarized in the following tables.

|Layer (type)               |  Output Shape             | Param #   |
|:---|:---|---:|
|cropping2d_1 (Cropping2D)  | (None, 90, 320, 3)        |0  |       
|lambda_1 (Lambda)          |  (None, 90, 320, 3)       | 0 |        
|conv2d_1 (Conv2D)           | (None, 86, 158, 16)      | 1216 |     
|leaky_re_lu_1 (LeakyReLU)    |(None, 86, 158, 16)      | 0     |    
|average_pooling2d_1 (Average |(None, 43, 79, 16)       | 0      |   
|conv2d_2 (Conv2D)            |(None, 39, 38, 24)       | 9624    |  
|leaky_re_lu_2 (LeakyReLU)    |(None, 39, 38, 24)       | 0        | 
|conv2d_3 (Conv2D)            |(None, 35, 17, 26)       | 15626     |
|leaky_re_lu_3 (LeakyReLU)    |(None, 35, 17, 26)       | 0         |
|conv2d_4 (Conv2D)            |(None, 31, 7, 30)        | 19530     |
|leaky_re_lu_4 (LeakyReLU)    |(None, 31, 7, 30)        | 0         |
|dropout_1 (Dropout)          |(None, 31, 7, 30)        | 0         |
|flatten_1 (Flatten)          |(None, 6510)             | 0         |
|dense_1 (Dense)              |(None, 30)               | 195330    |
|leaky_re_lu_5 (LeakyReLU)    |(None, 30)               | 0         |
|dense_2 (Dense)              |(None, 15)               | 465       |
|leaky_re_lu_6 (LeakyReLU)    |(None, 15)               | 0         |
|dense_3 (Dense)              |(None, 11)               | 176       |
|dense_4 (Dense)              |(None, 8)                | 96        |
|dense_5 (Dense)              |(None, 2)                | 18        |
|leaky_re_lu_7 (LeakyReLU)    |(None, 2)                | 0         |
|dense_6 (Dense)              |(None, 1)                | 3         |


|Total params| 242,084|
|:---|---:|
|Trainable params| 242,084 |
|Non-trainable params| 0 |


Here is a visualization of the architecture (note: visualizing the architecture is optional according to the project rubric)

![alt text][image1]

Fig. 2 Model visualization

#### 3. Creation of the Training Set & Training Process


The data I used for training network is the pre-recorded provided with the project. In order to generate enough data for training, I augment the original data to contain the following type of data:

- `image_center --> angle_center`: The standard center-drive data
- `image_right --> angle_right`: The augmented data for teaching the vehicle to recover from right side of the road. (`angle_right = angle_center - 0.3` increase the intension on turning left)
- `image_left --> angle_left`: The augmented data for teaching the vehicle to recover from left side of the road. (`angle_left = angle_center + 0.3` increase the intension on turning right)
- `Mirrored image_center --> -1*angle_center`: The flipped center-drive data for reusing the right-turn data on learning left-turn and vice-versa.
- `Mirrored image_right --> -1*angle_right`: The flipped image_right for reusing the right-turn data on learning left-turn and vice-versa.
- `Mirrored image_left --> -1*angle_left`: The flipped image_left for reusing the right-turn data on learning left-turn and vice-versa.

The first layer of the network integrate a `Cropping2D` layer for obtaining ROI. Fig. 3 shows one original center image, and Fig. 4 shows the cropped center image. Fig. 5, Fig. 6, Fig, 7, Fig, 8, and Fig. 9 shows the cropped `image_right`, `image_left`, `mirrored image_center`, `mirrored image_right`, and `mirrored image_left`, repectively.

![alt text][image2]
Fig. 3 Original center image (`image_center`)

![alt text][image3]
Fig. 4 Cropped center image (`image_center`)

![alt text][image4]
Fig. 5 Cropped right image (`image_right`)

![alt text][image5]
Fig. 6 Cropped right image (`image_left`)

![alt text][image6]
Fig. 7 Cropped and mirrored center image (`mirrored image_center`)

![alt text][image7]
Fig. 8 Cropped and mirrored right image (`mirrored image_right`)

![alt text][image8]
Fig. 9 Cropped and mirrored left image (`mirrored image_left`)




Then I repeated this process on track two in order to get more data points.

To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:

![alt text][image6]
![alt text][image7]

Etc ....

After the collection process, I had X number of data points. 



I then preprocessed this data by ...


I finally randomly shuffled the data set and put Y% of the data into a validation set. 

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary.
