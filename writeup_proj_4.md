# **Behavioral Cloning** 
**Behavioral Cloning Project**

The goals / steps of this project are the following:
* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./examples/placeholder.png "Model Visualization"
[image2]: ./examples/placeholder.png "Grayscaling"
[image3]: ./examples/placeholder_small.png "Recovery Image"
[image4]: ./examples/placeholder_small.png "Recovery Image"
[image5]: ./examples/placeholder_small.png "Recovery Image"
[image6]: ./examples/placeholder_small.png "Normal Image"
[image7]: ./examples/placeholder_small.png "Flipped Image"

## Rubric Points
Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_proj_4.md summarizes the results

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

#### 3. Submission code is usable and readable


Generator

The code in model.py uses a Python generator, if needed, to generate data for training rather than storing the training data in memory. The model.py code is clearly organized and comments are included where needed.




The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.


The model.py is dedicated to generate the model.h5, which is the NN model for generate the steering command for drive.py.
The model.py includes the following three key parts:
- Model definition
- Processing of traing and validation data
- Model training

**Model Definition**

The model is described and defined using Keras APIs in `create_model_B2()` function

**Processing Data**

The first step of data loading is to load the `driving_log.csv` file as a list. This file contains the files names of input images to NN and the output labels. After loading the `samples` list, I use `train_test_split()` from `sklearn.model_selection` module to ramdonly split the data set into `train_samples` and `validation_samples` in the ratio of 4:1 (80%, 20%).

Instead of directly loading all images into memory at once, I create a generator to load and process image data on the fly.

The basic structure of the data generator is shown in the following code block. Since the rule of generator is to continuously provide random batch to the optimizer, I remove the concept of `epoch` for simplification. This results in an architecture of single-producer-single-consumer type of queues. The producer load images and labels into the queues `images` and `angles`. The augmented data iares also generated by producer and are pushed into the queue, if there is any. The consumer checks if the size of the buffer is greater than `yield_size = 1.5*batch_size`. If the yield condition is satisfied, a portion of the data (`batch_size` of them) in the buffer will be popped and yield to the optimizer.

```python
# The batch generator with augmentaiton
def generator_aug(sample_list, batch_size=32, yield_size=None, data_path="."):
    #
    if yield_size is None:
        yield_size = int(batch_size*1.5)
    #
    num_samples = len(sample_list)
    images = []
    angles = []
    while True: # Eternal loop
        sample_list = sklearn.utils.shuffle(sample_list)
        #
        # Loop over all samples
        for sample in sample_list:
            current_path_center = data_path + '/IMG/' + sample[0].split('/')[-1]
            image_center = imageio.imread(current_path_center)
            angle_center = float(sample[3])
            # Center
            images.append(image_center)
            angles.append(angle_center)

            # yield
            if len(images) >= yield_size:
                # yield
                # Shuffle
                images, angles = sklearn.utils.shuffle(images, angles)
                # Convert to ndarray
                X_train = np.array(images[:batch_size])
                y_train = np.array(angles[:batch_size])
                # yield sklearn.utils.shuffle(X_train, y_train)
                yield (X_train, y_train)
                images = images[batch_size:]
                angles = angles[batch_size:]
            #

```
 
**Model Trainging**

Since my local computer sill sometimes shuted-down from over-heating, I construct a mechanism for resuming the training status from last unfinished training. This includes two parts:
- `ModelCheckpoint` callback to save temporary models
- `load_model` to load the save temporary model, if any.



### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

My model consists of four convolution layers, the filter kernal size are all 5x5 (model.py line 274, 278, 281, and 284). Each convolution layers are activated by Leaky-ReLU (model.py line 275, 279, 282, and 285). The input data are normalized by the second layer, which is Keras lambda layer performin the following function `lambda x: x / 255.0 - 0.5` (model.py line 272).


#### 2. Attempts to reduce overfitting in the model

The model contains a dropout layer at the last convolution layer (model.py line 286) and L2 regulizer for weight matrixes of all `Dense` layers.

The model was trained and validated on different data sets (`train_samples` and `validation_samples`) to varify that if the model is overfitted  (code line 49). However, the testing is using the realtime generated data by running the simulator and `drive.py` to how the car drive instead of test with pre-recorded dataset.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 25).

The model was trained by adam optimizer with all the parameters left to default (model.py line 298). The dropout rate for the model is set at 0.5 (model.py line 286). The batch size is set to 32 samples (model.py line 137); however, since the infinitively-looped generator is used, the `train_steps_epoch` is set arbitrarily to `100` (model.py line 357). The chosen of `train_steps_epoch` does not effect the quality of training; instead, it effect how offent the optimizer will validate the model with validation set and how offent the temporary model will be saved. I chose `100` so that the training step can be validated and saved offently.


#### 4. Appropriate training data

The data I used for training is basically the pre-recorded data provided by project. The data provided by project is a center-lane-driving record with center image, right-shifted image, and left-shifted image and the steering angle as label. Instead of collecting new data, I augmented the given dataset by utilizing the left/right images to generate recovery behaviors. Also, to further increase the data utilization, center images, left images, and right images are all mirrored horizontally. By doing so, the model can learn similar behavior for left and right turn or get more data for right/left turn from data of left/right turn.

The details of the data for training are described in the next section.






### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The overall strategy for deriving a model architecture was to ...

My first step was to use a convolution neural network model similar to the ... I thought this model might be appropriate because ...

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting. 

To combat the overfitting, I modified the model so that ...

Then I ... 

The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track... to improve the driving behavior in these cases, I ....

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Final Model Architecture

The final model architecture (model.py lines 18-24) consisted of a convolution neural network with the following layers and layer sizes ...

Here is a visualization of the architecture (note: visualizing the architecture is optional according to the project rubric)

![alt text][image1]

#### 3. Creation of the Training Set & Training Process

To capture good driving behavior, I first recorded two laps on track one using center lane driving. Here is an example image of center lane driving:

![alt text][image2]

I then recorded the vehicle recovering from the left side and right sides of the road back to center so that the vehicle would learn to .... These images show what a recovery looks like starting from ... :

![alt text][image3]
![alt text][image4]
![alt text][image5]

Then I repeated this process on track two in order to get more data points.

To augment the data sat, I also flipped images and angles thinking that this would ... For example, here is an image that has then been flipped:

![alt text][image6]
![alt text][image7]

Etc ....

After the collection process, I had X number of data points. I then preprocessed this data by ...


I finally randomly shuffled the data set and put Y% of the data into a validation set. 

I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was Z as evidenced by ... I used an adam optimizer so that manually training the learning rate wasn't necessary.
